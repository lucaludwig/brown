# -*- coding: utf-8 -*-
"""NLP_MA1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/113PHY919WUT0BNR5EV6oSPeemCBRWZtx

## Q1 Language Modelling
"""

import nltk
from nltk.corpus import brown
nltk.download('brown')
brown_words = brown.words()
brown_sents = brown.sents()

len(brown_words)
first_sentences = brown.sents()[0:300]
brown_words_1000 = brown.words()[0:1000]

#1.1
# Unigram

from collections import Counter

total_count = len(brown_words_1000)

counts = Counter(brown_words_1000)

# Compute the frequencies
for word in counts:
    counts[word] /= float(total_count)

#1.1
# Bigram

from nltk import bigrams, trigrams
from collections import Counter, defaultdict

#Lambda set to 1 as interpolation value
model_brown_bi = defaultdict(lambda: defaultdict(lambda: 1))
 
for sentence in first_sentences:
    for w2, w3 in bigrams(sentence, pad_right=True, pad_left=True):
        model_brown_bi[w2][w3] += 1

for w2 in model_brown_bi:
    total_count = float(sum(model_brown_bi[w2].values()))
    for w3 in model_brown_bi[w2]:
        if total_count == 0:
            model_brown_bi[w2][w3] = 0
        else:
            model_brown_bi[w2][w3] /= total_count

#1.1
#Trigram

#Lambda set to 1 as interpolation value
model_brown_tri = defaultdict(lambda: defaultdict(lambda: 1))

for sentence in brown.sents()[0:3]:
    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):
        model_brown_tri[(w1, w2)][w3] += 1

print(model_brown_tri.items())

for w1_w2 in model_brown_tri:
    total_count = float(sum(model_brown_tri[w1_w2].values()))
    for w3 in model_brown_tri[w1_w2]:
        if total_count == 0:
            model_brown_tri[w1_w2][w3] = 0
        else:
            model_brown_tri[w1_w2][w3] /= total_count

def combined_probability(w1, w2, w3, weight_uni=1/3, weight_bi=1/3, weight_tri=1/3):
  probability = weight_uni * counts[w3] + weight_bi * model_brown_bi[w2][w3] + weight_tri * model_brown_tri[w1_w2][w3]
  return probability

#1.3
#Trying out different weights of uni-, bi- and trigram

print(combined_probability('Fulton','primary','Court', 0.2, 0.2, 0.6))
print(combined_probability('Fulton','primary','Court', 0.4, 0.4, 0.2))
print(combined_probability('Fulton','primary','Court'))
print(combined_probability('Fulton','primary','Court', 0.6, 0.3, 0.1)) #highest probability

#1.4

#Random sentence construction

import random
import re

words = {}

for word in counts:
 words[word] = counts[word]

sentence = []
length = random.randint(4,13)

for i in range(length):
  position = random.randint(0,len(words))
  sentence.append(list(words)[position])

if len(model_brown_tri[sentence[-1],sentence[-2]]) > 0:
  res = str(list((dict(model_brown_tri[sentence[-1],sentence[-2]]))))
  res = " ".join(re.findall("[a-zA-Z]+", res))
  sentence.append(res)

if len(model_brown_tri[sentence[-2],sentence[-1]]) > 0:
  res = str(list((dict(model_brown_tri[sentence[-1],sentence[-2]]))))
  res = " ".join(re.findall("[a-zA-Z]+", res))
  sentence.append(res)

sentence = " ".join(sentence)
print(sentence)

"""## Q2 Naive Bayes"""

import re
import nltk
from nltk.corpus import stopwords
from prettytable import PrettyTable
from functools import reduce
from operator import mul

# Prepare train and test docs
train_docs = [("the actor gives a convincing, charismatic performance as the multifaceted","entertaining"),
              ("Spielberg gives us a visually spicy and historically accurate real life story","entertaining"),
              ("His innovative mind entertains us now and will continue to entertain generations to come","entertaining"),
              ("Unfortunately, the film has two major flaws, one in the disastrous ending","boring"),
              ("If director actually thought this movie was worth anything", "boring"),
              ("His efforts seem fruitless, creates drama where drama shouldn’t be","boring")
             ]

entertaining_docs = [doc[0] for doc in train_docs if doc[1] == "entertaining"]
boring_docs = [doc[0] for doc in train_docs if doc[1] == "boring"]

test_doc = "film is a innovative drama, entertains, but disastrous ending"

n_entertaining_docs = len(entertaining_docs) # count(S,C=entertaining)
n_boring_docs = len(boring_docs) # count(S,C=boring)
n_all_docs = len(train_docs) # count(S,C=entertaining)+count(S,C=boring)

# Compute prior P(C)
P_entertaining = n_entertaining_docs/n_all_docs
P_boring = n_boring_docs/n_all_docs

print("The prior for the \"entertaining\" class is", P_entertaining)
print("The prior for the \"boring\" class is", P_boring)

# Create bag of words and compute cardinalities
all_words = []
entertaining_words = []
boring_words = []

for (doc, cat) in train_docs:
    temp = re.split(r'[ ,][ ]*',doc)
    for word in temp:
        all_words.append(word.lower())
        if cat == "entertaining":
            entertaining_words.append(word.lower())
        elif cat == "boring":
            boring_words.append(word.lower())
            
n_entertaining_words = len(entertaining_words) # count(w,C=entertaining)
n_boring_words = len(boring_words) # count(w,C=boring)
n_all_words = len(all_words) # |V|=count(w,C=entertaining)+count(w,C=boring)

nltk.download('stopwords')
# Exclude stop words
stop_words = set(stopwords.words('english'))
filtered_out = set([word for word in all_words if word in stop_words])

print("Words from the train docs that appear in stop words:",filtered_out)

all_words = [word for word in all_words if not word in stop_words]
entertaining_words = [word for word in entertaining_words if not word in stop_words]
boring_words = [word for word in boring_words if not word in stop_words]

# compute likelihood probability for each word from test doc P(w|C) with add-1 smoothing
test_words = [word for word in re.split(r'[ ,][ ]*',test_doc) if not word in stop_words]
P_w_entertaining = []
P_w_boring = []

for word in test_words:
    #if word in all_words:
        temp_entertaining = (entertaining_words.count(word)+1)/(n_entertaining_words+n_all_words) # +1 for add-1 smoothing
        temp_boring = (boring_words.count(word)+1)/(n_boring_words+n_all_words) # +1 for add-1 smoothing
        P_w_entertaining.append((word, temp_entertaining))
        P_w_boring.append((word,temp_boring))

# printing tables of individual likelihood probabilities for each class
tab = PrettyTable()
tab.title = 'Likelihoods for the \"entertaining\" class (without stop words)'
tab.field_names = ['word','likelihood']

for likelihood in P_w_entertaining:
    tab.add_row(likelihood)

print(tab)

tab = PrettyTable()
tab.title = 'Likelihoods for the \"boring\" class (without stop words)'
tab.field_names = ['word','likelihood']

for likelihood in P_w_boring:
    tab.add_row(likelihood)

print(tab)

# Compute (overall) likelihood probability of the test doc for each class P(S|C)
P_S_entertaining = reduce(mul,[x[1] for x in P_w_entertaining])
P_S_boring = reduce(mul,[x[1] for x in P_w_boring])

# Compute posterior probability of test doc for each class P(C|S)
likelihood_entertaining = P_entertaining *P_S_entertaining # P(C=entertaining)*P(S|C=entertaining)
likelihood_boring = P_boring *P_S_boring # P(C=boring)*P(S|C=boring)

print("The posterior probability of the test doc given the \"entertaining\" class is", P_S_entertaining)
print("The posterior probability of the test doc given the \"boring\" class is", P_S_boring)

# Produce class prediction for the test doc
if likelihood_entertaining > likelihood_boring:
    print("The output indicates a higher probability of the \"entertaining\” class for the test doc.")
elif likelihood_entertaining < likelihood_boring:
    print("The output indicates a higher probability of the \"boring” class for the test doc.")
else:
    print("The output allows for no clear classification of the test doc.")